# -*- coding: utf-8 -*-
# Copyright (c) 2025 Salvador E. Tropea
# Copyright (c) 2025 Instituto Nacional de Tecnolog√Øa Industrial
# License: GPL-3.0
# Project: ComfyUI-AudioBatch
# From code generated by Gemini 2.5 Pro
import torch
import torchaudio
import torchaudio.transforms as T
from typing import Optional, Dict, Any
from .utils.aligner import AudioBatchAligner
from .utils.logger import main_logger
from .utils.misc import parse_time_to_seconds, parse_note_to_frequency

logger = main_logger
BASE_CATEGORY = "audio"
BATCH_CATEGORY = "batch"
CONV_CATEGORY = "conversion"
MANIPULATION_CATEGORY = "manipulation"
GEN_CATEGORY = "generation"


class AudioBatch:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio1": ("AUDIO", {"tooltip": "The first audio input. Can be a single audio item or a batch"}),
                "audio2": ("AUDIO", {"tooltip": "The second audio input. Can be a single audio item or a batch"}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_batch",)
    FUNCTION = "batch_audio"
    CATEGORY = BASE_CATEGORY + "/" + BATCH_CATEGORY
    DESCRIPTION = "Audio batch creator"
    UNIQUE_NAME = "SET_AudioBatch"
    DISPLAY_NAME = "Batch Audios"

    def batch_audio(self, audio1: dict, audio2: dict):
        # 1. Instantiate the aligner
        aligner = AudioBatchAligner(audio1, audio2)
        # 2. Get the aligned waveforms
        aligned_wf1, aligned_wf2, target_sr = aligner.get_aligned_waveforms()
        # 3. Concatenate
        batched_waveform_final = torch.cat((aligned_wf1, aligned_wf2), dim=0)

        logger.info(f"Final batched audio: shape={batched_waveform_final.shape}, sr={target_sr}")

        output_audio = {"waveform": batched_waveform_final, "sample_rate": target_sr}
        return (output_audio,)


class SelectAudioFromBatch:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio_batch": ("AUDIO",),  # Expects {'waveform': (B, C, N), 'sample_rate': int}
                "index": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 0xffffffffffffffff,  # Effectively unbounded, but UI might cap
                    "step": 1,
                    "display": "number",
                    "tootip": "The 0-based index of the audio stream to select from the batch"
                }),
                "behavior_out_of_range": (["silence_original_length", "silence_fixed_length", "error"], {
                    "default": "silence_original_length",
                    "tootip": ("silence_original_length: Output silent audio with the same channel count and duration as "
                               "items in the original batch.\n"
                               "silence_fixed_length: Output silent audio with a duration specified by "
                               " silence_duration_seconds.\n"
                               "error: Raise an error (which will halt the workflow and display an error in ComfyUI)")
                }),
                "silence_duration_seconds": ("FLOAT", {  # Only used if behavior is "silence_fixed_length"
                    "default": 1.0,
                    "min": 0.01,
                    "max": 3600.0,  # 1 hour
                    "step": 0.1,
                    "display": "number",
                    "tootip": "The duration of the silent audio if `behavior_out_of_range` is set to `silence_fixed_length`"
                }),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("selected_audio",)
    FUNCTION = "select_audio"
    CATEGORY = BASE_CATEGORY + "/" + BATCH_CATEGORY
    DESCRIPTION = "Selects an audio from a batch"
    UNIQUE_NAME = "SET_SelectAudioFromBatch"
    DISPLAY_NAME = "Select Audio from Batch"

    def select_audio(self, audio_batch: dict, index: int, behavior_out_of_range: str, silence_duration_seconds: float):

        waveform_batch = audio_batch['waveform']  # (B, C, N)
        sample_rate = audio_batch['sample_rate']

        batch_size, num_channels, num_samples_per_item = waveform_batch.shape

        logger.debug(f"Input audio batch: shape={waveform_batch.shape}, sr={sample_rate}, index={index}")
        logger.debug(f"Out of range behavior: {behavior_out_of_range}, silence duration: {silence_duration_seconds}s")

        selected_waveform = None

        if 0 <= index < batch_size:
            # Valid index, select the audio
            # Slicing with index:index+1 keeps the batch dimension, resulting in (1, C, N)
            selected_waveform = waveform_batch[index:index+1, :, :]
            logger.info(f"Selected audio at index {index}. Shape: {selected_waveform.shape}")
        else:
            # Index is out of range
            msg = f"Index {index} is out of range for batch of size {batch_size}."
            logger.warning(msg)

            if behavior_out_of_range == "error":
                raise ValueError(msg)
            elif behavior_out_of_range == "silence_original_length":
                logger.info(f"Outputting silence with original length: {num_samples_per_item} samples, "
                            f"{num_channels} channels.")
                selected_waveform = torch.zeros((1, num_channels if num_channels > 0 else 1, num_samples_per_item),
                                                dtype=waveform_batch.dtype, device=waveform_batch.device)
            elif behavior_out_of_range == "silence_fixed_length":
                silence_samples = int(silence_duration_seconds * sample_rate)
                if silence_samples <= 0:  # Ensure positive sample count
                    silence_samples = 1
                    logger.warning(f"Calculated silence samples is <=0 ({silence_samples} from "
                                   f"{silence_duration_seconds}s). Using 1 sample.")
                logger.info(f"Outputting silence with fixed length: {silence_samples} samples, {num_channels} channels.")
                selected_waveform = torch.zeros((1, num_channels if num_channels > 0 else 1, silence_samples),
                                                dtype=waveform_batch.dtype, device=waveform_batch.device)

        if selected_waveform is None:  # Should not happen if logic above is complete
            logger.error("Selected waveform is None unexpectedly. Defaulting to silence.")
            selected_waveform = torch.zeros((1, num_channels if num_channels > 0 else 1, 1),
                                            dtype=waveform_batch.dtype, device=waveform_batch.device)

        output_audio = {
            "waveform": selected_waveform,  # Shape (1, C, N_selected)
            "sample_rate": sample_rate
        }

        return (output_audio,)


class AudioChannelConverter:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "channel_conversion": (["keep", "stereo_to_mono", "mono_to_stereo", "force_mono", "force_stereo"],
                                       {"default": "keep",
                                        "tooltip": "keep: maintain same channels,\n"
                                                   "stereo_to_mono/force_mono: 1 channel,\n"
                                                   "mono_to_stereo/force_stereo: 2 channels"}),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "convert_channels"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Converts audio channels (mono/stereo/multi-channel handling)."
    UNIQUE_NAME = "SET_AudioChannelConverter"
    DISPLAY_NAME = "Audio Channel Converter"

    def convert_channels(self, audio: dict, channel_conversion: str):
        waveform = audio['waveform']  # (B, C, T)
        sample_rate = audio['sample_rate']

        original_batch_size, original_channels, original_samples = waveform.shape
        logger.debug(f"Input audio: {original_batch_size}B, {original_channels}C, {original_samples}T @ {sample_rate}Hz")
        logger.debug(f"Channel conversion mode: {channel_conversion}")

        output_waveform = waveform.clone()  # Start with a copy

        if channel_conversion == "keep":
            # No change needed, but log if > 2 channels
            if original_channels > 2:
                logger.warning(f"Channel mode 'keep': Input has {original_channels} channels. Outputting all "
                               f"{original_channels} channels.")
            # output_waveform remains as is

        elif channel_conversion == "stereo_to_mono" or channel_conversion == "force_mono":
            if original_channels == 1:
                logger.debug("Input is already mono. No change for stereo_to_mono/force_mono.")
                # output_waveform remains as is
            else:  # Stereo or Multi-channel to Mono
                if original_channels > 2 and channel_conversion == "stereo_to_mono":
                    logger.warning(f"Channel mode 'stereo_to_mono': Input has {original_channels} channels. "
                                   "Averaging all to mono.")
                elif channel_conversion == "force_mono":
                    logger.info(f"Channel mode 'force_mono': Input has {original_channels} channels. Averaging all to mono.")
                # Average across the channel dimension (dim=1)
                output_waveform = torch.mean(waveform, dim=1, keepdim=True)
                logger.debug(f"Converted to mono. New shape: {output_waveform.shape}")

        elif channel_conversion == "mono_to_stereo" or channel_conversion == "force_stereo":
            if original_channels == 2:
                logger.debug("Input is already stereo. No change for mono_to_stereo/force_stereo.")
                # output_waveform remains as is
            elif original_channels == 1:  # Mono to Stereo
                # Duplicate the mono channel
                output_waveform = waveform.repeat(1, 2, 1)
                logger.debug(f"Converted mono to stereo by duplication. New shape: {output_waveform.shape}")
            else:  # Multi-channel (>2) to Stereo
                if channel_conversion == "mono_to_stereo":
                    logger.warning(f"Channel mode 'mono_to_stereo': Input has {original_channels} channels. "
                                   "Taking the first channel and duplicating it to create stereo.")
                elif channel_conversion == "force_stereo":
                    logger.info(f"Channel mode 'force_stereo': Input has {original_channels} channels. "
                                "Taking the first channel and duplicating it to create stereo.")
                output_waveform = waveform[:, 0:1, :].repeat(1, 2, 1)  # Take first channel, make it (B,1,T), then repeat
                logger.debug(f"Converted {original_channels}ch to stereo. New shape: {output_waveform.shape}")
        else:
            logger.error(f"Unknown channel_conversion mode: {channel_conversion}. Returning original.")
            # output_waveform remains as is (original waveform)

        processed_audio_dict = {"waveform": output_waveform, "sample_rate": sample_rate}
        return (processed_audio_dict,)


class AudioResampler:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "target_sample_rate": ("INT", {
                    "default": 0, "min": 0, "max": 192000, "step": 100, "tooltip": "Output sample rate, 0 is same as input"}),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "resample_audio"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Resamples audio to a target sample rate using torchaudio."
    UNIQUE_NAME = "SET_AudioResampler"
    DISPLAY_NAME = "Audio Resampler"

    def resample_audio(self, audio: dict, target_sample_rate: int):
        waveform = audio['waveform']  # (B, C, T)
        original_sample_rate = audio['sample_rate']

        logger.debug(f"Input audio SR: {original_sample_rate}Hz, Target SR: {target_sample_rate}Hz")

        if target_sample_rate == 0 or target_sample_rate == original_sample_rate:
            logger.info(f"Target sample rate ({target_sample_rate}Hz) is 0 or matches original ({original_sample_rate}Hz). "
                        "Skipping resampling.")
            return (audio,)  # Return original audio dict

        try:
            # Ensure waveform is on the CPU for torchaudio transforms if it might be on GPU
            # Though many torchaudio ops support GPU tensors. Resample does.
            # For consistency or if issues arise:
            # device = waveform.device
            # waveform_cpu = waveform.cpu()
            # resampler = T.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate).to(device)
            # resampled_waveform = resampler(waveform)
            resampler = T.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate,
                                   dtype=waveform.dtype,  # Preserve dtype
                                   lowpass_filter_width=24
                                   ).to(waveform.device)  # Perform resampling on the tensor's current device

            resampled_waveform = resampler(waveform)
            logger.info(f"Resampling audio from {original_sample_rate}Hz to {target_sample_rate}Hz. "
                        f"Original shape: {waveform.shape}, New shape: {resampled_waveform.shape}")
            processed_audio_dict = {"waveform": resampled_waveform, "sample_rate": target_sample_rate}
            return (processed_audio_dict,)

        except Exception as e:
            logger.error(f"Error during resampling: {e}", exc_info=True)
            # Fallback: return original audio if resampling fails
            return (audio,)


class AudioProcessAdvanced:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "channel_conversion": (["keep", "stereo_to_mono", "mono_to_stereo", "force_mono", "force_stereo"],
                                       {"default": "keep",
                                        "tooltip": "keep: maintain same channels,\n"
                                                   "stereo_to_mono/force_mono: 1 channel,\n"
                                                   "mono_to_stereo/force_stereo: 2 channels"}),
                "target_sample_rate": ("INT", {"default": 0, "min": 0, "max": 192000, "step": 100.,
                                               "tooltip": "Output sample rate, 0 is same as input"}),
            },
        }
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "process_audio"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Applies channel conversion and resampling to audio."
    UNIQUE_NAME = "SET_AudioChannelConvResampler"
    DISPLAY_NAME = "Audio Channel Conv and Resampler"

    def process_audio(self, audio: dict, channel_conversion: str, target_sample_rate: int):
        # Instantiate helper classes (or move their logic directly here)
        channel_converter_node = AudioChannelConverter()
        resampler_node = AudioResampler()

        # 1. Channel Conversion
        (audio_after_channels,) = channel_converter_node.convert_channels(audio, channel_conversion)

        # 2. Resampling
        (audio_after_resample,) = resampler_node.resample_audio(audio_after_channels, target_sample_rate)

        return (audio_after_resample,)


class AudioInfo:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
            },
        }

    RETURN_TYPES = ("AUDIO", "INT", "INT", "INT", "INT")
    RETURN_NAMES = ("audio_bypass", "batch_size", "channels", "num_samples", "sample_rate")
    FUNCTION = "show_info"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Shows information about the audio."
    UNIQUE_NAME = "SET_AudioInfo"
    DISPLAY_NAME = "Audio Information"

    def show_info(self, audio: dict):
        wav = audio['waveform']  # (B, C, T)
        sample_rate = audio['sample_rate']
        return audio, wav.shape[0], wav.shape[1], wav.shape[2], sample_rate


class AudioForceChannels:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "channels": ("INT", {"default": 0, "min": 0, "max": 2}),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio",)
    FUNCTION = "force_channels"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Forces the number of channels. 0 means keep same."
    UNIQUE_NAME = "SET_AudioForceChannels"
    DISPLAY_NAME = "Audio Force Channels"
    CHANNELS_TO_MODE = ["keep", "force_mono", "force_stereo"]

    def force_channels(self, audio: dict, channels: int):
        return AudioChannelConverter().convert_channels(audio, self.CHANNELS_TO_MODE[channels])


class AudioCut:
    """ From https://github.com/christian-byrne/audio-separation-nodes-comfyui/ """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "start_time": ("STRING", {
                    "default": "0:00",
                    "tooltip": "Start time in HH:MM:SS.ss format, or just a float."
                }),
                "end_time": ("STRING", {
                    "default": "1:00",
                    "tooltip": "End time in HH:MM:SS.ss format, or just a float."
                }),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "cut"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Cuts a portion of the input audio. Can use seconds or HH:MM:SS.ss."
    UNIQUE_NAME = "SET_AudioCut"
    DISPLAY_NAME = "Audio Cut"

    def cut(self, audio: dict, start_time: str, end_time: str):
        waveform = audio["waveform"]
        sample_rate = audio["sample_rate"]
        length = waveform.shape[-1] - 1

        start_frame = int(parse_time_to_seconds(start_time) * sample_rate)
        start_frame = max(0, min(start_frame, length))

        end_frame = int(parse_time_to_seconds(end_time) * sample_rate)
        end_frame = max(0, min(end_frame, length))

        logger.debug(f"Cutting audio from {start_frame} to {end_frame} (SR: {sample_rate})")
        if start_frame > end_frame:
            raise ValueError("Audio Cut: Start time must be smaller than end time and both be within the audio length.")

        return ({"waveform": waveform[..., start_frame:end_frame],
                 "sample_rate": sample_rate},)


class AudioBlend:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio1": ("AUDIO", {"tooltip": "The first audio input (batch supported)."}),
                "gain1": ("FLOAT", {"default": 1.0, "min": -10.0, "max": 10.0, "step": 0.01,
                                    "tooltip": "Volume gain for the first audio. Can be negative to subtract."}),
                "gain2": ("FLOAT", {"default": 1.0, "min": -10.0, "max": 10.0, "step": 0.01,
                                    "tooltip": "Volume gain for the second audio. Can be negative to subtract."}),
            },
            "optional": {
                "audio2": ("AUDIO", {"tooltip": "The second audio input (optional, batch supported)."}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "blend_audio"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Blends two audio inputs by applying gain and adding them. Supports batches."
    UNIQUE_NAME = "SET_AudioBlend"
    DISPLAY_NAME = "Audio Blend"

    def blend_audio(self, audio1: dict, gain1: float, gain2: float, audio2: Optional[Dict[str, Any]] = None):
        if audio2 is None:
            # Handle the simple case where audio2 is not provided
            logger.info(f"Blending audio1 only with gain {gain1}.")
            blended_waveform = audio1['waveform'] * gain1
            return ({"waveform": blended_waveform, "sample_rate": audio1['sample_rate']},)

        # If audio2 is provided, align both audio inputs
        # 1. Instantiate the aligner with the logger
        aligner = AudioBatchAligner(audio1, audio2)

        # 2. Get the aligned waveforms
        aligned_wf1, aligned_wf2, target_sr = aligner.get_aligned_waveforms()

        b1, c, n = aligned_wf1.shape
        b2 = aligned_wf2.shape[0]

        # 3. Perform the blend operation on aligned tensors
        target_batch_size = max(b1, b2)
        blended_waveform = torch.zeros(target_batch_size, c, n, dtype=aligned_wf1.dtype, device=aligned_wf1.device)

        logger.info(f"Blending {b1} items from audio1 with {b2} items from audio2 into a batch of {target_batch_size}.")

        # Add the scaled audio streams to the output tensor
        # This correctly handles mismatched batch sizes by only adding where data exists.
        blended_waveform[:b1] += aligned_wf1 * gain1
        blended_waveform[:b2] += aligned_wf2 * gain2

        output_audio = {"waveform": blended_waveform, "sample_rate": target_sr}
        return (output_audio,)


class AudioTestSignalGenerator:
    WAVEFORM_TYPES = ["sine", "square", "sawtooth", "triangle", "sweep",
                      "white_noise", "pink_noise", "brownian_noise",
                      "impulse", "silence"]

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "waveform_type": (cls.WAVEFORM_TYPES, {"default": "sine"}),
                "frequency": ("FLOAT", {"default": 440.0, "min": 0.0, "max": 22050.0, "step": 1.0,
                                        "tooltip": "Frequency in Hz for periodic waveforms (or start frequency for sweep)."}),
                "frequency_end": ("FLOAT", {"default": 880.0, "min": 0.0, "max": 22050.0, "step": 1.0,
                                            "tooltip": "End frequency in Hz for 'sweep' waveform type."}),
                "amplitude": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.01,
                                        "tooltip": "Amplitude of the signal (0.0 to 1.0)."}),
                "dc_offset": ("FLOAT", {"default": 0.0, "min": -1.0, "max": 1.0, "step": 0.01,
                                        "tooltip": "DC offset to add to the signal."}),
                "phase": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 360.0, "step": 1.0,
                                    "tooltip": "Phase offset in degrees (0-360)."}),
                # "duration_seconds": ("FLOAT", {"default": 3.0, "min": 0.01, "max": 3600.0, "step": 0.1}),
                "duration": ("STRING", {
                    "default": "3.0",
                    "tooltip": "Duration time in HH:MM:SS.ss format, or just a float for seconds."
                }),
                "sample_rate": ("INT", {"default": 44100, "min": 1, "max": 192000, "step": 100}),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 64, "step": 1}),
                "channels": ("INT", {"default": 1, "min": 1, "max": 8, "step": 1,
                                     "tooltip": "Number of audio channels (1=mono, 2=stereo, etc.)."}),
            },
            "optional": {  # To allow for deterministic noise generation
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "generate_signal"
    CATEGORY = BASE_CATEGORY + "/" + GEN_CATEGORY
    DESCRIPTION = "Generates various audio test signals like sine waves, noise, and sweeps."
    UNIQUE_NAME = "SET_AudioTestSignalGenerator"
    DISPLAY_NAME = "Audio Test Signal Generator"

    def generate_signal(self, waveform_type: str, frequency: float, frequency_end: float,
                        amplitude: float, dc_offset: float, phase: float,
                        duration: str, sample_rate: int,
                        batch_size: int, channels: int, seed: int = 0):

        duration_seconds = parse_time_to_seconds(duration)
        num_samples = int(duration_seconds * sample_rate)
        if num_samples <= 0:
            logger.warning("Duration and sample rate result in 0 or negative samples. Outputting empty audio.")
            waveform = torch.empty((batch_size, channels, 0), dtype=torch.float32)
            return ({"waveform": waveform, "sample_rate": sample_rate},)

        # Create the time vector `t` for one channel
        t = torch.linspace(0., duration_seconds, num_samples)

        # Convert phase from degrees to radians
        phase_rad = torch.deg2rad(torch.tensor(phase))

        waveform_1d = None
        waveform_multichannel = None

        if waveform_type == "sine":
            waveform_1d = torch.sin(2 * torch.pi * frequency * t + phase_rad)

        elif waveform_type == "square":
            # A square wave is the sign of a sine wave
            waveform_1d = torch.sign(torch.sin(2 * torch.pi * frequency * t + phase_rad))
            # Avoid pure 0s for a cleaner square wave if sine hits exactly 0
            waveform_1d[waveform_1d == 0] = 1.0

        elif waveform_type == "sawtooth":
            # A sawtooth wave is related to the modulo of the time component
            waveform_1d = 2 * (t * frequency - torch.floor(0.5 + t * frequency))

        elif waveform_type == "triangle":
            # A triangle wave can be derived from the absolute value of a sawtooth
            sawtooth = 2 * (t * frequency - torch.floor(0.5 + t * frequency))
            waveform_1d = 2 * torch.abs(sawtooth) - 1

        elif waveform_type == "sweep":
            # Linear frequency sweep (chirp) from `frequency` to `frequency_end`
            logger.debug(f"Generating linear sweep from {frequency}Hz to {frequency_end}Hz.")
            # Instantaneous frequency: f(t) = f_start + (f_end - f_start) * (t / duration)
            # Phase is the integral of instantaneous frequency
            f_start = frequency
            f_end = frequency_end
            k = (f_end - f_start) / duration_seconds  # Sweep rate
            # Phase = 2 * pi * (f_start * t + (k/2) * t^2)
            phase_sweep = 2 * torch.pi * (f_start * t + (k / 2.0) * (t ** 2))
            waveform_1d = torch.sin(phase_sweep + phase_rad)

        elif "noise" in waveform_type:
            generator = torch.Generator().manual_seed(seed)
            noise_shape = (batch_size, channels, num_samples)

            # Use torchaudio's colored_noise if available for pink/brownian
            if waveform_type != "white_noise" and hasattr(torchaudio.functional, 'colored_noise'):
                try:
                    exponent = -1 if waveform_type == "pink_noise" else -2
                    # Reshape for torchaudio function and then back
                    flat_noise = torchaudio.functional.colored_noise(
                        size=(batch_size * channels, num_samples),
                        exponent=exponent,
                        generator=generator
                    )
                    waveform_multichannel = flat_noise.reshape(noise_shape)
                except Exception as e:
                    logger.warning(f"torchaudio.functional.colored_noise failed ({e}). Falling back to simpler methods.")
                    waveform_multichannel = None  # Signal fallback

            if waveform_multichannel is None:  # Fallback or white_noise
                white_noise = torch.rand(noise_shape, generator=generator) * 2 - 1
                if waveform_type == "brownian_noise":
                    brownian_unscaled = torch.cumsum(white_noise, dim=2)
                    max_abs, _ = torch.max(torch.abs(brownian_unscaled), dim=2, keepdim=True)
                    waveform_multichannel = brownian_unscaled / (max_abs + 1e-9)
                else:  # For white_noise or as a fallback for pink_noise
                    if waveform_type == "pink_noise":
                        logger.warning("Using white noise as a fallback for pink noise generation.")
                    waveform_multichannel = white_noise

        elif waveform_type == "impulse":
            waveform_1d = torch.zeros(num_samples)
            if num_samples > 0:
                waveform_1d[0] = 1.0  # Single sample impulse at the beginning

        elif waveform_type == "silence":
            # The amplitude and dc_offset will be applied correctly later.
            waveform_1d = torch.zeros(num_samples)

        else:
            logger.error(f"Unknown waveform type: {waveform_type}. Defaulting to silence.")
            waveform_1d = torch.zeros(num_samples)

        # Apply amplitude and DC offset to periodic/single-channel waveforms
        if waveform_1d is not None:
            waveform_1d = waveform_1d * amplitude + dc_offset
            # Expand to the correct number of channels and batch size
            # (1, num_samples) -> (1, 1, num_samples) -> (batch_size, channels, num_samples)
            waveform_multichannel = waveform_1d.unsqueeze(0).unsqueeze(0).repeat(batch_size, channels, 1)
        elif 'noise' not in waveform_type:  # if waveform_multichannel was not created in noise block
            logger.error("Waveform generation failed. Outputting silence.")
            waveform_multichannel = torch.zeros((batch_size, channels, num_samples))

        # Final clip to ensure [-1, 1] range, as some operations might exceed it slightly
        assert waveform_multichannel is not None, "Waveform should have been created by now"
        final_waveform = torch.clamp(waveform_multichannel, -1.0, 1.0)

        logger.info(f"Generated '{waveform_type}' signal. Shape: {final_waveform.shape}, SR: {sample_rate}Hz")

        output_audio = {
            "waveform": final_waveform,
            "sample_rate": sample_rate
        }

        return (output_audio,)


class AudioMusicalNote:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "note": ("STRING", {
                    "default": "A",
                    "tooltip": "The musical note in American notation (e.g., C, F#, Gb, 'D sharp'). Case-insensitive."
                }),
                "octave": ("INT", {
                    "default": 4,
                    "min": 0,
                    "max": 8,  # Standard piano range is roughly 0-8
                    "step": 1,
                    "tooltip": "The octave number (e.g., 4 corresponds to middle C's octave)."
                }),
            }
        }

    RETURN_TYPES = ("FLOAT",)
    RETURN_NAMES = ("frequency",)
    FUNCTION = "get_frequency"
    CATEGORY = BASE_CATEGORY + "/" + GEN_CATEGORY
    DESCRIPTION = "Converts a musical note and octave to its corresponding frequency in Hz."
    UNIQUE_NAME = "SET_AudioMusicalNote"
    DISPLAY_NAME = "Audio Musical Note"

    def get_frequency(self, note: str, octave: int):
        try:
            frequency = parse_note_to_frequency(note, octave)
            logger.info(f"Parsed note '{note}{octave}' to {frequency:.2f} Hz.")
            return (frequency,)
        except (ValueError, TypeError) as e:
            # If the user enters an invalid note, log it as a warning
            # and return a default safe frequency (e.g., A4) to avoid crashing.
            # A UI warning could also be sent if this were a generator.
            logger.error(f"Error parsing note: {e}. Defaulting to 440.0 Hz.")
            return (440.0,)


class AudioJoin2Channels:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio_left": ("AUDIO", {"tooltip": "The audio signal for the left channel. Will be converted to mono."}),
                "audio_right": ("AUDIO", {"tooltip": "The audio signal for the right channel. Will be converted to mono."}),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "join_channels"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Joins two audio signals (L/R) into a single stereo audio signal."
    UNIQUE_NAME = "SET_AudioJoin2Channels"
    DISPLAY_NAME = "Audio Join 2 Channels"

    def join_channels(self, audio_left: dict, audio_right: dict):
        # 1. Force both inputs to be mono to ensure they represent single channels.
        # We reuse the logic from AudioChannelConverter.
        channel_converter = AudioChannelConverter()
        (mono_left_audio,) = channel_converter.convert_channels(audio_left, "force_mono")
        (mono_right_audio,) = channel_converter.convert_channels(audio_right, "force_mono")

        # 2. Align the two mono signals in terms of sample rate and length.
        # This reuses the robust logic from AudioBatchAligner.
        # The output of the aligner will have channels=1 since both inputs are mono.
        aligner = AudioBatchAligner(mono_left_audio, mono_right_audio)
        aligned_left_wf, aligned_right_wf, target_sr = aligner.get_aligned_waveforms()

        # aligned_left_wf is (B_left, 1, N), aligned_right_wf is (B_right, 1, N)

        # 3. Handle mismatched batch sizes.
        b_left, b_right = aligned_left_wf.shape[0], aligned_right_wf.shape[0]
        target_batch_size = max(b_left, b_right)

        # Create final waveforms with the target batch size, repeating the last item if needed.
        final_left_wf = aligned_left_wf
        if b_left < target_batch_size:
            last_left_item = aligned_left_wf[-1:, :, :]  # (1, 1, N)
            repeats_needed = target_batch_size - b_left
            final_left_wf = torch.cat([aligned_left_wf, last_left_item.repeat(repeats_needed, 1, 1)], dim=0)

        final_right_wf = aligned_right_wf
        if b_right < target_batch_size:
            last_right_item = aligned_right_wf[-1:, :, :]  # (1, 1, N)
            repeats_needed = target_batch_size - b_right
            final_right_wf = torch.cat([aligned_right_wf, last_right_item.repeat(repeats_needed, 1, 1)], dim=0)

        # At this point, final_left_wf and final_right_wf are both (target_batch_size, 1, N)

        # 4. Concatenate the mono channels into a stereo signal.
        # `torch.cat` along the channel dimension (dim=1).
        stereo_waveform = torch.cat((final_left_wf, final_right_wf), dim=1)  # (B, 2, N)

        logger.info(f"Joined L/R channels into stereo audio. Final shape: {stereo_waveform.shape}, SR: {target_sr}")

        output_audio = {
            "waveform": stereo_waveform,
            "sample_rate": target_sr
        }
        return (output_audio,)


class AudioSplit2Channels:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO", {"tooltip": "A stereo audio signal to split into separate channels."}),
            },
        }

    RETURN_TYPES = ("AUDIO", "AUDIO")
    RETURN_NAMES = ("audio_left", "audio_right")
    FUNCTION = "split_channels"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Splits a stereo audio signal into two separate mono audio signals (L/R)."
    UNIQUE_NAME = "SET_AudioSplit2Channels"
    DISPLAY_NAME = "Audio Split 2 Channels"

    def split_channels(self, audio: dict):
        waveform = audio['waveform']  # (B, C, N)
        sample_rate = audio['sample_rate']

        num_channels = waveform.shape[1]

        # 1. Validate that the input is stereo.
        if num_channels != 2:
            msg = f"Input audio must be stereo (2 channels) to be split. Got {num_channels} channels."
            logger.error(msg)
            # This is a hard requirement, so raising an error is appropriate.
            raise ValueError(msg)

        # 2. Slice the tensor along the channel dimension.
        # Slicing with [:, 0:1, :] keeps the channel dimension as 1, so the output is (B, 1, N)
        left_channel_wf = waveform[:, 0:1, :]
        right_channel_wf = waveform[:, 1:2, :]

        logger.info(f"Split stereo audio into L/R channels. Output shape for each: {left_channel_wf.shape}")

        # 3. Package each channel into its own ComfyUI AUDIO dict.
        audio_left = {
            "waveform": left_channel_wf,
            "sample_rate": sample_rate
        }
        audio_right = {
            "waveform": right_channel_wf,
            "sample_rate": sample_rate
        }

        return (audio_left, audio_right)


class AudioConcatenate:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio1": ("AUDIO", {"tooltip": "The first audio clip (or batch)."}),
                "audio2": ("AUDIO", {"tooltip": "The second audio clip (or batch) to append."}),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "concatenate_audio"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Concatenates two audio signals end-to-end in time."
    UNIQUE_NAME = "SET_AudioConcatenate"
    DISPLAY_NAME = "Audio Concatenate"

    def concatenate_audio(self, audio1: dict, audio2: dict):
        # 1. Align the two audio inputs. This will unify their sample rate and channel count.
        # It will also pad their *lengths* to be equal, which is not what we want for concatenation.
        # We will use the aligned waveforms but ignore the padding by using their original lengths
        # after resampling.

        aligner = AudioBatchAligner(audio1, audio2)
        # aligned_wf1 is (B1, C_target, N_target), aligned_wf2 is (B2, C_target, N_target)
        aligned_wf1, aligned_wf2, target_sr = aligner.get_aligned_waveforms()

        # 2. Determine the original lengths *after resampling* to know how much to take from each.
        n1_orig = audio1['waveform'].shape[2]
        n2_orig = audio2['waveform'].shape[2]
        n1_after_resample = (n1_orig if audio1['sample_rate'] == target_sr else
                             int(n1_orig * (target_sr / audio1['sample_rate'])))
        n2_after_resample = (n2_orig if audio2['sample_rate'] == target_sr else
                             int(n2_orig * (target_sr / audio2['sample_rate'])))

        # Take the un-padded, aligned data from each waveform
        wf1_to_concat = aligned_wf1[..., :n1_after_resample]
        wf2_to_concat = aligned_wf2[..., :n2_after_resample]

        # 3. Handle mismatched batch sizes by repeating the last item.
        b1, b2 = wf1_to_concat.shape[0], wf2_to_concat.shape[0]
        if b1 != b2:
            if b1 < b2:
                last_item = wf1_to_concat[-1:, :, :]  # Keep batch dim
                repeats_needed = b2 - b1
                wf1_to_concat = torch.cat([wf1_to_concat, last_item.repeat(repeats_needed, 1, 1)], dim=0)
            else:  # b2 < b1
                last_item = wf2_to_concat[-1:, :, :]
                repeats_needed = b1 - b2
                wf2_to_concat = torch.cat([wf2_to_concat, last_item.repeat(repeats_needed, 1, 1)], dim=0)

        # Now both wf1_to_concat and wf2_to_concat are (max(B1,B2), C_target, N_relevant)

        # 4. Concatenate along the time/samples dimension (dim=2)
        concatenated_waveform = torch.cat((wf1_to_concat, wf2_to_concat), dim=2)

        logger.info(f"Concatenated audio. Final shape: {concatenated_waveform.shape}, SR: {target_sr}")

        output_audio = {
            "waveform": concatenated_waveform,
            "sample_rate": target_sr
        }
        return (output_audio,)
