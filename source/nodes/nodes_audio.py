# -*- coding: utf-8 -*-
# Copyright (c) 2025 Salvador E. Tropea
# Copyright (c) 2025 Instituto Nacional de Tecnolog√Øa Industrial
# License: GPL-3.0
# Project: ComfyUI-AudioBatch
# From code generated by Gemini 2.5 Pro
import os
import torch
import torchaudio
import torchaudio.transforms as T
from typing import Optional, Dict, Any
from .utils.aligner import AudioBatchAligner
from .utils.logger import main_logger
from .utils.misc import parse_time_to_seconds, parse_note_to_frequency
from .utils.downmix import spectral_downmix
from .utils.downloader import download_model
from .utils.comfy_notification import send_toast_notification
try:
    from folder_paths import get_input_directory   # To get the ComfyUI input directory
except ModuleNotFoundError:
    # No ComfyUI, this is a test environment
    def get_input_directory():
        return ""

logger = main_logger
BASE_CATEGORY = "audio"
BATCH_CATEGORY = "batch"
CONV_CATEGORY = "conversion"
MANIPULATION_CATEGORY = "manipulation"
GEN_CATEGORY = "generation"
IO_CATEGORY = "io"
DOWNMIX_OPTIONS = (["average", "standard_gain", "spectral"],
                   {"default": "standard_gain",
                    "tooltip": ("Method for stereo/multi-channel to mono conversion:\n"
                                "- average: Simple average ((L+R)/2). Can reduce volume.\n"
                                "- standard_gain: Sums channels with -3dB gain (0.707). "
                                "Better preserves perceived loudness."
                                "- spectral: Averages frequency magnitudes to prevent phase cancellation.")})
DOWNMIX_NFFT = ("INT", {
                  "default": 2048,
                  "min": 256,
                  "max": 8192,  # Powers of 2 are typical
                  "step": 256,
                  "tooltip": ("FFT size for spectral downmixing. "
                              "Higher values give better frequency resolution but worse time resolution.")
               })
DOWNMIX_HOP = ("INT", {
                  "default": 512,
                  "min": 64,
                  "max": 4096,
                  "step": 64,
                  "tooltip": "Hop length for STFT. Typically n_fft / 4. Controls time resolution."
              })


class AudioBatch:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio1": ("AUDIO", {"tooltip": "The first audio input. Can be a single audio item or a batch"}),
                "audio2": ("AUDIO", {"tooltip": "The second audio input. Can be a single audio item or a batch"}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_batch",)
    FUNCTION = "batch_audio"
    CATEGORY = BASE_CATEGORY + "/" + BATCH_CATEGORY
    DESCRIPTION = "Audio batch creator"
    UNIQUE_NAME = "SET_AudioBatch"
    DISPLAY_NAME = "Batch Audios"

    def batch_audio(self, audio1: dict, audio2: dict):
        # 1. Instantiate the aligner
        aligner = AudioBatchAligner(audio1, audio2)
        # 2. Get the aligned waveforms
        aligned_wf1, aligned_wf2, target_sr = aligner.get_aligned_waveforms()
        # 3. Concatenate
        batched_waveform_final = torch.cat((aligned_wf1, aligned_wf2), dim=0)

        logger.info(f"Final batched audio: shape={batched_waveform_final.shape}, sr={target_sr}")

        output_audio = {"waveform": batched_waveform_final, "sample_rate": target_sr}
        return (output_audio,)


class SelectAudioFromBatch:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio_batch": ("AUDIO",),  # Expects {'waveform': (B, C, N), 'sample_rate': int}
                "index": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 0xffffffffffffffff,  # Effectively unbounded, but UI might cap
                    "step": 1,
                    "display": "number",
                    "tootip": "The 0-based index of the audio stream to select from the batch"
                }),
                "behavior_out_of_range": (["silence_original_length", "silence_fixed_length", "error"], {
                    "default": "silence_original_length",
                    "tootip": ("silence_original_length: Output silent audio with the same channel count and duration as "
                               "items in the original batch.\n"
                               "silence_fixed_length: Output silent audio with a duration specified by "
                               " silence_duration_seconds.\n"
                               "error: Raise an error (which will halt the workflow and display an error in ComfyUI)")
                }),
                "silence_duration_seconds": ("FLOAT", {  # Only used if behavior is "silence_fixed_length"
                    "default": 1.0,
                    "min": 0.01,
                    "max": 3600.0,  # 1 hour
                    "step": 0.1,
                    "display": "number",
                    "tootip": "The duration of the silent audio if `behavior_out_of_range` is set to `silence_fixed_length`"
                }),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("selected_audio",)
    FUNCTION = "select_audio"
    CATEGORY = BASE_CATEGORY + "/" + BATCH_CATEGORY
    DESCRIPTION = "Selects an audio from a batch"
    UNIQUE_NAME = "SET_SelectAudioFromBatch"
    DISPLAY_NAME = "Select Audio from Batch"

    def select_audio(self, audio_batch: dict, index: int, behavior_out_of_range: str, silence_duration_seconds: float):

        waveform_batch = audio_batch['waveform']  # (B, C, N)
        sample_rate = audio_batch['sample_rate']

        batch_size, num_channels, num_samples_per_item = waveform_batch.shape

        logger.debug(f"Input audio batch: shape={waveform_batch.shape}, sr={sample_rate}, index={index}")
        logger.debug(f"Out of range behavior: {behavior_out_of_range}, silence duration: {silence_duration_seconds}s")

        selected_waveform = None

        if 0 <= index < batch_size:
            # Valid index, select the audio
            # Slicing with index:index+1 keeps the batch dimension, resulting in (1, C, N)
            selected_waveform = waveform_batch[index:index+1, :, :]
            logger.info(f"Selected audio at index {index}. Shape: {selected_waveform.shape}")
        else:
            # Index is out of range
            msg = f"Index {index} is out of range for batch of size {batch_size}."
            logger.warning(msg)

            if behavior_out_of_range == "error":
                raise ValueError(msg)
            elif behavior_out_of_range == "silence_original_length":
                logger.info(f"Outputting silence with original length: {num_samples_per_item} samples, "
                            f"{num_channels} channels.")
                selected_waveform = torch.zeros((1, num_channels if num_channels > 0 else 1, num_samples_per_item),
                                                dtype=waveform_batch.dtype, device=waveform_batch.device)
            elif behavior_out_of_range == "silence_fixed_length":
                silence_samples = int(silence_duration_seconds * sample_rate)
                if silence_samples <= 0:  # Ensure positive sample count
                    silence_samples = 1
                    logger.warning(f"Calculated silence samples is <=0 ({silence_samples} from "
                                   f"{silence_duration_seconds}s). Using 1 sample.")
                logger.info(f"Outputting silence with fixed length: {silence_samples} samples, {num_channels} channels.")
                selected_waveform = torch.zeros((1, num_channels if num_channels > 0 else 1, silence_samples),
                                                dtype=waveform_batch.dtype, device=waveform_batch.device)

        if selected_waveform is None:  # Should not happen if logic above is complete
            logger.error("Selected waveform is None unexpectedly. Defaulting to silence.")
            selected_waveform = torch.zeros((1, num_channels if num_channels > 0 else 1, 1),
                                            dtype=waveform_batch.dtype, device=waveform_batch.device)

        output_audio = {
            "waveform": selected_waveform,  # Shape (1, C, N_selected)
            "sample_rate": sample_rate
        }

        return (output_audio,)


class AudioChannelConverter:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "channel_conversion": (["keep", "stereo_to_mono", "mono_to_stereo", "force_mono", "force_stereo"],
                                       {"default": "keep",
                                        "tooltip": "keep: maintain same channels,\n"
                                                   "stereo_to_mono/force_mono: 1 channel,\n"
                                                   "mono_to_stereo/force_stereo: 2 channels"}),
                "downmix_method": DOWNMIX_OPTIONS,
            },
            "optional": {
                "n_fft": DOWNMIX_NFFT,
                "hop_length": DOWNMIX_HOP,
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "convert_channels"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Converts audio channels (mono/stereo/multi-channel handling)."
    UNIQUE_NAME = "SET_AudioChannelConverter"
    DISPLAY_NAME = "Audio Channel Converter"

    def convert_channels(self, audio: dict, channel_conversion: str, downmix_method: str, n_fft: int = 2048,
                         hop_length: int = 512):
        waveform = audio['waveform']  # (B, C, T)
        sample_rate = audio['sample_rate']

        original_batch_size, original_channels, original_samples = waveform.shape
        logger.debug(f"Input audio: {original_batch_size}B, {original_channels}C, {original_samples}T @ {sample_rate}Hz")
        logger.debug(f"Channel conversion mode: {channel_conversion}")

        output_waveform = waveform.clone()  # Start with a copy

        if channel_conversion == "keep":
            # No change needed, but log if > 2 channels
            if original_channels > 2:
                logger.warning(f"Channel mode 'keep': Input has {original_channels} channels. Outputting all "
                               f"{original_channels} channels.")
            # output_waveform remains as is

        elif channel_conversion == "stereo_to_mono" or channel_conversion == "force_mono":
            if original_channels == 1:
                logger.debug("Input is already mono. No change for stereo_to_mono/force_mono.")
                # output_waveform remains as is
            else:  # Stereo or Multi-channel to Mono
                if original_channels > 2 and channel_conversion == "stereo_to_mono":
                    logger.warning(f"Channel mode 'stereo_to_mono': Input has {original_channels} channels. "
                                   "Averaging all to mono.")
                logger.info(f"Converting {original_channels} channels to mono using '{downmix_method}' method.")

                if downmix_method == "average":
                    # Simple average across the channel dimension
                    output_waveform = torch.mean(waveform, dim=1, keepdim=True)
                elif downmix_method == "standard_gain":
                    # Sum channels and apply gain compensation.
                    # This is equivalent to (L*0.707 + R*0.707) for stereo.
                    # For multi-channel, it sums all channels and divides by sqrt(num_channels).
                    gain = 1.0 / (original_channels ** 0.5)
                    logger.debug(f"Applying downmix gain of {gain:.4f} (1/sqrt({original_channels}))")
                    # Sum along channel dimension, then apply gain. keepdim=True for (B,1,N) shape.
                    output_waveform = torch.sum(waveform, dim=1, keepdim=True) * gain
                elif downmix_method == "spectral":
                    output_waveform = spectral_downmix(waveform, n_fft=n_fft, hop_length=hop_length)

                logger.debug(f"Converted to mono. New shape: {output_waveform.shape}")

        elif channel_conversion == "mono_to_stereo" or channel_conversion == "force_stereo":
            if original_channels == 2:
                logger.debug("Input is already stereo. No change for mono_to_stereo/force_stereo.")
                # output_waveform remains as is
            elif original_channels == 1:  # Mono to Stereo
                # Duplicate the mono channel
                output_waveform = waveform.repeat(1, 2, 1)
                logger.debug(f"Converted mono to stereo by duplication. New shape: {output_waveform.shape}")
            else:  # Multi-channel (>2) to Stereo
                if channel_conversion == "mono_to_stereo":
                    logger.warning(f"Channel mode 'mono_to_stereo': Input has {original_channels} channels. "
                                   "Taking the first channel and duplicating it to create stereo.")
                elif channel_conversion == "force_stereo":
                    logger.info(f"Channel mode 'force_stereo': Input has {original_channels} channels. "
                                "Taking the first channel and duplicating it to create stereo.")
                output_waveform = waveform[:, 0:1, :].repeat(1, 2, 1)  # Take first channel, make it (B,1,T), then repeat
                logger.debug(f"Converted {original_channels}ch to stereo. New shape: {output_waveform.shape}")
        else:
            logger.error(f"Unknown channel_conversion mode: {channel_conversion}. Returning original.")
            # output_waveform remains as is (original waveform)

        processed_audio_dict = {"waveform": output_waveform, "sample_rate": sample_rate}
        return (processed_audio_dict,)


class AudioResampler:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "target_sample_rate": ("INT", {
                    "default": 0, "min": 0, "max": 192000, "step": 100, "tooltip": "Output sample rate, 0 is same as input"}),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "resample_audio"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Resamples audio to a target sample rate using torchaudio."
    UNIQUE_NAME = "SET_AudioResampler"
    DISPLAY_NAME = "Audio Resampler"

    def resample_audio(self, audio: dict, target_sample_rate: int):
        waveform = audio['waveform']  # (B, C, T)
        original_sample_rate = audio['sample_rate']

        logger.debug(f"Input audio SR: {original_sample_rate}Hz, Target SR: {target_sample_rate}Hz")

        if target_sample_rate == 0 or target_sample_rate == original_sample_rate:
            logger.info(f"Target sample rate ({target_sample_rate}Hz) is 0 or matches original ({original_sample_rate}Hz). "
                        "Skipping resampling.")
            return (audio,)  # Return original audio dict

        try:
            # Ensure waveform is on the CPU for torchaudio transforms if it might be on GPU
            # Though many torchaudio ops support GPU tensors. Resample does.
            # For consistency or if issues arise:
            # device = waveform.device
            # waveform_cpu = waveform.cpu()
            # resampler = T.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate).to(device)
            # resampled_waveform = resampler(waveform)
            resampler = T.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate,
                                   dtype=waveform.dtype,  # Preserve dtype
                                   lowpass_filter_width=24
                                   ).to(waveform.device)  # Perform resampling on the tensor's current device

            resampled_waveform = resampler(waveform)
            logger.info(f"Resampling audio from {original_sample_rate}Hz to {target_sample_rate}Hz. "
                        f"Original shape: {waveform.shape}, New shape: {resampled_waveform.shape}")
            processed_audio_dict = {"waveform": resampled_waveform, "sample_rate": target_sample_rate}
            return (processed_audio_dict,)

        except Exception as e:
            logger.error(f"Error during resampling: {e}", exc_info=True)
            # Fallback: return original audio if resampling fails
            return (audio,)


class AudioProcessAdvanced:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "channel_conversion": (["keep", "stereo_to_mono", "mono_to_stereo", "force_mono", "force_stereo"],
                                       {"default": "keep",
                                        "tooltip": "keep: maintain same channels,\n"
                                                   "stereo_to_mono/force_mono: 1 channel,\n"
                                                   "mono_to_stereo/force_stereo: 2 channels"}),
                "target_sample_rate": ("INT", {"default": 0, "min": 0, "max": 192000, "step": 100.,
                                               "tooltip": "Output sample rate, 0 is same as input"}),
                "downmix_method": DOWNMIX_OPTIONS,
            },
            "optional": {
                "n_fft": DOWNMIX_NFFT,
                "hop_length": DOWNMIX_HOP,
            }
        }
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "process_audio"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Applies channel conversion and resampling to audio."
    UNIQUE_NAME = "SET_AudioChannelConvResampler"
    DISPLAY_NAME = "Audio Channel Conv and Resampler"

    def process_audio(self, audio: dict, channel_conversion: str, target_sample_rate: int, downmix_method: str,
                      n_fft: int = 2048, hop_length: int = 512):
        # Instantiate helper classes (or move their logic directly here)
        channel_converter_node = AudioChannelConverter()
        resampler_node = AudioResampler()

        # 1. Channel Conversion
        (audio_after_channels,) = channel_converter_node.convert_channels(audio, channel_conversion, downmix_method,
                                                                          n_fft=n_fft, hop_length=hop_length)

        # 2. Resampling
        (audio_after_resample,) = resampler_node.resample_audio(audio_after_channels, target_sample_rate)

        return (audio_after_resample,)


class AudioInfo:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
            },
        }

    RETURN_TYPES = ("AUDIO", "INT", "INT", "INT", "INT", "TORCH_TENSOR", "TORCH_TENSOR", "TORCH_TENSOR")
    RETURN_NAMES = ("audio_bypass", "batch_size", "channels", "num_samples", "sample_rate", "mean", "std", "peak")
    FUNCTION = "show_info"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Shows information about the audio."
    UNIQUE_NAME = "SET_AudioInfo"
    DISPLAY_NAME = "Audio Information"

    def show_info(self, audio: dict):
        wav = audio['waveform']  # (B, C, T)
        sample_rate = audio['sample_rate']

        ref = wav.mean(1, keepdim=True)
        mean = ref.mean(dim=2, keepdim=True)
        std = ref.std(dim=2, keepdim=True)

        max_val, _ = torch.max(torch.abs(wav), dim=2, keepdim=True)
        max_val, _ = torch.max(max_val, dim=1, keepdim=True)  # (B, 1, 1)

        return audio, wav.shape[0], wav.shape[1], wav.shape[2], sample_rate, mean.squeeze(), std.squeeze(), max_val.squeeze()


class AudioForceChannels:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "channels": ("INT", {"default": 0, "min": 0, "max": 2}),
                "downmix_method": DOWNMIX_OPTIONS,
            },
            "optional": {
                "n_fft": DOWNMIX_NFFT,
                "hop_length": DOWNMIX_HOP,
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio",)
    FUNCTION = "force_channels"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Forces the number of channels. 0 means keep same."
    UNIQUE_NAME = "SET_AudioForceChannels"
    DISPLAY_NAME = "Audio Force Channels"
    CHANNELS_TO_MODE = ["keep", "force_mono", "force_stereo"]

    def force_channels(self, audio: dict, channels: int, downmix_method: str, n_fft: int = 2048, hop_length: int = 512):
        return AudioChannelConverter().convert_channels(audio, self.CHANNELS_TO_MODE[channels], downmix_method, n_fft=n_fft,
                                                        hop_length=hop_length)


class AudioCut:
    """ From https://github.com/christian-byrne/audio-separation-nodes-comfyui/ """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "start_time": ("STRING", {
                    "default": "0:00",
                    "tooltip": "Start time in HH:MM:SS.ss format, or just a float."
                }),
                "end_time": ("STRING", {
                    "default": "1:00",
                    "tooltip": "End time in HH:MM:SS.ss format, or just a float."
                }),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "cut"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Cuts a portion of the input audio. Can use seconds or HH:MM:SS.ss."
    UNIQUE_NAME = "SET_AudioCut"
    DISPLAY_NAME = "Audio Cut"

    def cut(self, audio: dict, start_time: str, end_time: str):
        waveform = audio["waveform"]
        sample_rate = audio["sample_rate"]
        length = waveform.shape[-1] - 1

        start_frame = int(parse_time_to_seconds(start_time) * sample_rate)
        start_frame = max(0, min(start_frame, length))

        end_frame = int(parse_time_to_seconds(end_time) * sample_rate)
        end_frame = max(0, min(end_frame, length))

        logger.debug(f"Cutting audio from {start_frame} to {end_frame} (SR: {sample_rate})")
        if start_frame > end_frame:
            raise ValueError("Audio Cut: Start time must be smaller than end time and both be within the audio length.")

        return ({"waveform": waveform[..., start_frame:end_frame],
                 "sample_rate": sample_rate},)


class AudioBlend:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio1": ("AUDIO", {"tooltip": "The first audio input (batch supported)."}),
                "gain1": ("FLOAT", {"default": 1.0, "min": -10.0, "max": 10.0, "step": 0.01,
                                    "tooltip": "Volume gain for the first audio. Can be negative to subtract."}),
                "gain2": ("FLOAT", {"default": 1.0, "min": -10.0, "max": 10.0, "step": 0.01,
                                    "tooltip": "Volume gain for the second audio. Can be negative to subtract."}),
            },
            "optional": {
                "audio2": ("AUDIO", {"tooltip": "The second audio input (optional, batch supported)."}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "blend_audio"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Blends two audio inputs by applying gain and adding them. Supports batches."
    UNIQUE_NAME = "SET_AudioBlend"
    DISPLAY_NAME = "Audio Blend"

    def blend_audio(self, audio1: dict, gain1: float, gain2: float, audio2: Optional[Dict[str, Any]] = None):
        if audio2 is None:
            # Handle the simple case where audio2 is not provided
            logger.info(f"Blending audio1 only with gain {gain1}.")
            blended_waveform = audio1['waveform'] * gain1
            return ({"waveform": blended_waveform, "sample_rate": audio1['sample_rate']},)

        # If audio2 is provided, align both audio inputs
        # 1. Instantiate the aligner with the logger
        aligner = AudioBatchAligner(audio1, audio2)

        # 2. Get the aligned waveforms
        aligned_wf1, aligned_wf2, target_sr = aligner.get_aligned_waveforms()

        b1, c, n = aligned_wf1.shape
        b2 = aligned_wf2.shape[0]

        # 3. Perform the blend operation on aligned tensors
        target_batch_size = max(b1, b2)
        blended_waveform = torch.zeros(target_batch_size, c, n, dtype=aligned_wf1.dtype, device=aligned_wf1.device)

        logger.info(f"Blending {b1} items from audio1 with {b2} items from audio2 into a batch of {target_batch_size}.")

        # Add the scaled audio streams to the output tensor
        # This correctly handles mismatched batch sizes by only adding where data exists.
        blended_waveform[:b1] += aligned_wf1 * gain1
        blended_waveform[:b2] += aligned_wf2 * gain2

        output_audio = {"waveform": blended_waveform, "sample_rate": target_sr}
        return (output_audio,)


class AudioTestSignalGenerator:
    WAVEFORM_TYPES = ["sine", "square", "sawtooth", "triangle", "sweep",
                      "white_noise", "pink_noise", "brownian_noise",
                      "impulse", "silence"]

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "waveform_type": (cls.WAVEFORM_TYPES, {"default": "sine"}),
                "frequency": ("FLOAT", {"default": 440.0, "min": 0.0, "max": 22050.0, "step": 1.0,
                                        "tooltip": "Frequency in Hz for periodic waveforms (or start frequency for sweep)."}),
                "frequency_end": ("FLOAT", {"default": 880.0, "min": 0.0, "max": 22050.0, "step": 1.0,
                                            "tooltip": "End frequency in Hz for 'sweep' waveform type."}),
                "amplitude": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.01,
                                        "tooltip": "Amplitude of the signal (0.0 to 1.0)."}),
                "dc_offset": ("FLOAT", {"default": 0.0, "min": -1.0, "max": 1.0, "step": 0.01,
                                        "tooltip": "DC offset to add to the signal."}),
                "phase": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 360.0, "step": 1.0,
                                    "tooltip": "Phase offset in degrees (0-360)."}),
                # "duration_seconds": ("FLOAT", {"default": 3.0, "min": 0.01, "max": 3600.0, "step": 0.1}),
                "duration": ("STRING", {
                    "default": "3.0",
                    "tooltip": "Duration time in HH:MM:SS.ss format, or just a float for seconds."
                }),
                "sample_rate": ("INT", {"default": 44100, "min": 1, "max": 192000, "step": 100}),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 64, "step": 1}),
                "channels": ("INT", {"default": 1, "min": 1, "max": 8, "step": 1,
                                     "tooltip": "Number of audio channels (1=mono, 2=stereo, etc.)."}),
            },
            "optional": {  # To allow for deterministic noise generation
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "generate_signal"
    CATEGORY = BASE_CATEGORY + "/" + GEN_CATEGORY
    DESCRIPTION = "Generates various audio test signals like sine waves, noise, and sweeps."
    UNIQUE_NAME = "SET_AudioTestSignalGenerator"
    DISPLAY_NAME = "Audio Test Signal Generator"

    def generate_signal(self, waveform_type: str, frequency: float, frequency_end: float,
                        amplitude: float, dc_offset: float, phase: float,
                        duration: str, sample_rate: int,
                        batch_size: int, channels: int, seed: int = 0):

        duration_seconds = parse_time_to_seconds(duration)
        num_samples = int(duration_seconds * sample_rate)
        if num_samples <= 0:
            logger.warning("Duration and sample rate result in 0 or negative samples. Outputting empty audio.")
            waveform = torch.empty((batch_size, channels, 0), dtype=torch.float32)
            return ({"waveform": waveform, "sample_rate": sample_rate},)

        # Create the time vector `t` for one channel
        t = torch.linspace(0., duration_seconds, num_samples)

        # Convert phase from degrees to radians
        phase_rad = torch.deg2rad(torch.tensor(phase))

        waveform_1d = None
        waveform_multichannel = None

        if waveform_type == "sine":
            waveform_1d = torch.sin(2 * torch.pi * frequency * t + phase_rad)

        elif waveform_type == "square":
            # A square wave is the sign of a sine wave
            waveform_1d = torch.sign(torch.sin(2 * torch.pi * frequency * t + phase_rad))
            # Avoid pure 0s for a cleaner square wave if sine hits exactly 0
            waveform_1d[waveform_1d == 0] = 1.0

        elif waveform_type == "sawtooth":
            # A sawtooth wave is related to the modulo of the time component
            waveform_1d = 2 * (t * frequency - torch.floor(0.5 + t * frequency))

        elif waveform_type == "triangle":
            # A triangle wave can be derived from the absolute value of a sawtooth
            sawtooth = 2 * (t * frequency - torch.floor(0.5 + t * frequency))
            waveform_1d = 2 * torch.abs(sawtooth) - 1

        elif waveform_type == "sweep":
            # Linear frequency sweep (chirp) from `frequency` to `frequency_end`
            logger.debug(f"Generating linear sweep from {frequency}Hz to {frequency_end}Hz.")
            # Instantaneous frequency: f(t) = f_start + (f_end - f_start) * (t / duration)
            # Phase is the integral of instantaneous frequency
            f_start = frequency
            f_end = frequency_end
            k = (f_end - f_start) / duration_seconds  # Sweep rate
            # Phase = 2 * pi * (f_start * t + (k/2) * t^2)
            phase_sweep = 2 * torch.pi * (f_start * t + (k / 2.0) * (t ** 2))
            waveform_1d = torch.sin(phase_sweep + phase_rad)

        elif "noise" in waveform_type:
            generator = torch.Generator().manual_seed(seed)
            noise_shape = (batch_size, channels, num_samples)

            # Use torchaudio's colored_noise if available for pink/brownian
            if waveform_type != "white_noise" and hasattr(torchaudio.functional, 'colored_noise'):
                try:
                    exponent = -1 if waveform_type == "pink_noise" else -2
                    # Reshape for torchaudio function and then back
                    flat_noise = torchaudio.functional.colored_noise(
                        size=(batch_size * channels, num_samples),
                        exponent=exponent,
                        generator=generator
                    )
                    waveform_multichannel = flat_noise.reshape(noise_shape)
                except Exception as e:
                    logger.warning(f"torchaudio.functional.colored_noise failed ({e}). Falling back to simpler methods.")
                    waveform_multichannel = None  # Signal fallback

            if waveform_multichannel is None:  # Fallback or white_noise
                white_noise = torch.rand(noise_shape, generator=generator) * 2 - 1
                if waveform_type == "brownian_noise":
                    brownian_unscaled = torch.cumsum(white_noise, dim=2)
                    max_abs, _ = torch.max(torch.abs(brownian_unscaled), dim=2, keepdim=True)
                    waveform_multichannel = brownian_unscaled / (max_abs + 1e-9)
                else:  # For white_noise or as a fallback for pink_noise
                    if waveform_type == "pink_noise":
                        logger.warning("Using white noise as a fallback for pink noise generation.")
                    waveform_multichannel = white_noise

        elif waveform_type == "impulse":
            waveform_1d = torch.zeros(num_samples)
            if num_samples > 0:
                waveform_1d[0] = 1.0  # Single sample impulse at the beginning

        elif waveform_type == "silence":
            # The amplitude and dc_offset will be applied correctly later.
            waveform_1d = torch.zeros(num_samples)

        else:
            logger.error(f"Unknown waveform type: {waveform_type}. Defaulting to silence.")
            waveform_1d = torch.zeros(num_samples)

        # Apply amplitude and DC offset to periodic/single-channel waveforms
        if waveform_1d is not None:
            waveform_1d = waveform_1d * amplitude + dc_offset
            # Expand to the correct number of channels and batch size
            # (1, num_samples) -> (1, 1, num_samples) -> (batch_size, channels, num_samples)
            waveform_multichannel = waveform_1d.unsqueeze(0).unsqueeze(0).repeat(batch_size, channels, 1)
        elif 'noise' not in waveform_type:  # if waveform_multichannel was not created in noise block
            logger.error("Waveform generation failed. Outputting silence.")
            waveform_multichannel = torch.zeros((batch_size, channels, num_samples))

        # Final clip to ensure [-1, 1] range, as some operations might exceed it slightly
        assert waveform_multichannel is not None, "Waveform should have been created by now"
        final_waveform = torch.clamp(waveform_multichannel, -1.0, 1.0)

        logger.info(f"Generated '{waveform_type}' signal. Shape: {final_waveform.shape}, SR: {sample_rate}Hz")

        output_audio = {
            "waveform": final_waveform,
            "sample_rate": sample_rate
        }

        return (output_audio,)


class AudioMusicalNote:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "note": ("STRING", {
                    "default": "A",
                    "tooltip": "The musical note in American notation (e.g., C, F#, Gb, 'D sharp'). Case-insensitive."
                }),
                "octave": ("INT", {
                    "default": 4,
                    "min": 0,
                    "max": 8,  # Standard piano range is roughly 0-8
                    "step": 1,
                    "tooltip": "The octave number (e.g., 4 corresponds to middle C's octave)."
                }),
            }
        }

    RETURN_TYPES = ("FLOAT",)
    RETURN_NAMES = ("frequency",)
    FUNCTION = "get_frequency"
    CATEGORY = BASE_CATEGORY + "/" + GEN_CATEGORY
    DESCRIPTION = "Converts a musical note and octave to its corresponding frequency in Hz."
    UNIQUE_NAME = "SET_AudioMusicalNote"
    DISPLAY_NAME = "Audio Musical Note"

    def get_frequency(self, note: str, octave: int):
        try:
            frequency = parse_note_to_frequency(note, octave)
            logger.info(f"Parsed note '{note}{octave}' to {frequency:.2f} Hz.")
            return (frequency,)
        except (ValueError, TypeError) as e:
            # If the user enters an invalid note, log it as a warning
            # and return a default safe frequency (e.g., A4) to avoid crashing.
            # A UI warning could also be sent if this were a generator.
            logger.error(f"Error parsing note: {e}. Defaulting to 440.0 Hz.")
            return (440.0,)


class AudioJoin2Channels:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio_left": ("AUDIO", {"tooltip": "The audio signal for the left channel. Will be converted to mono."}),
                "audio_right": ("AUDIO", {"tooltip": "The audio signal for the right channel. Will be converted to mono."}),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "join_channels"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Joins two audio signals (L/R) into a single stereo audio signal."
    UNIQUE_NAME = "SET_AudioJoin2Channels"
    DISPLAY_NAME = "Audio Join 2 Channels"

    def join_channels(self, audio_left: dict, audio_right: dict):
        # 1. Force both inputs to be mono to ensure they represent single channels.
        # We reuse the logic from AudioChannelConverter.
        channel_converter = AudioChannelConverter()
        (mono_left_audio,) = channel_converter.convert_channels(audio_left, "force_mono", "average")
        (mono_right_audio,) = channel_converter.convert_channels(audio_right, "force_mono", "average")

        # 2. Align the two mono signals in terms of sample rate and length.
        # This reuses the robust logic from AudioBatchAligner.
        # The output of the aligner will have channels=1 since both inputs are mono.
        aligner = AudioBatchAligner(mono_left_audio, mono_right_audio)
        aligned_left_wf, aligned_right_wf, target_sr = aligner.get_aligned_waveforms()

        # aligned_left_wf is (B_left, 1, N), aligned_right_wf is (B_right, 1, N)

        # 3. Handle mismatched batch sizes.
        b_left, b_right = aligned_left_wf.shape[0], aligned_right_wf.shape[0]
        target_batch_size = max(b_left, b_right)

        # Create final waveforms with the target batch size, repeating the last item if needed.
        final_left_wf = aligned_left_wf
        if b_left < target_batch_size:
            last_left_item = aligned_left_wf[-1:, :, :]  # (1, 1, N)
            repeats_needed = target_batch_size - b_left
            final_left_wf = torch.cat([aligned_left_wf, last_left_item.repeat(repeats_needed, 1, 1)], dim=0)

        final_right_wf = aligned_right_wf
        if b_right < target_batch_size:
            last_right_item = aligned_right_wf[-1:, :, :]  # (1, 1, N)
            repeats_needed = target_batch_size - b_right
            final_right_wf = torch.cat([aligned_right_wf, last_right_item.repeat(repeats_needed, 1, 1)], dim=0)

        # At this point, final_left_wf and final_right_wf are both (target_batch_size, 1, N)

        # 4. Concatenate the mono channels into a stereo signal.
        # `torch.cat` along the channel dimension (dim=1).
        stereo_waveform = torch.cat((final_left_wf, final_right_wf), dim=1)  # (B, 2, N)

        logger.info(f"Joined L/R channels into stereo audio. Final shape: {stereo_waveform.shape}, SR: {target_sr}")

        output_audio = {
            "waveform": stereo_waveform,
            "sample_rate": target_sr
        }
        return (output_audio,)


class AudioSplit2Channels:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO", {"tooltip": "A stereo audio signal to split into separate channels."}),
            },
        }

    RETURN_TYPES = ("AUDIO", "AUDIO")
    RETURN_NAMES = ("audio_left", "audio_right")
    FUNCTION = "split_channels"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Splits a stereo audio signal into two separate mono audio signals (L/R)."
    UNIQUE_NAME = "SET_AudioSplit2Channels"
    DISPLAY_NAME = "Audio Split 2 Channels"

    def split_channels(self, audio: dict):
        waveform = audio['waveform']  # (B, C, N)
        sample_rate = audio['sample_rate']

        num_channels = waveform.shape[1]

        # 1. Validate that the input is stereo.
        if num_channels != 2:
            msg = f"Input audio must be stereo (2 channels) to be split. Got {num_channels} channels."
            logger.error(msg)
            # This is a hard requirement, so raising an error is appropriate.
            raise ValueError(msg)

        # 2. Slice the tensor along the channel dimension.
        # Slicing with [:, 0:1, :] keeps the channel dimension as 1, so the output is (B, 1, N)
        left_channel_wf = waveform[:, 0:1, :]
        right_channel_wf = waveform[:, 1:2, :]

        logger.info(f"Split stereo audio into L/R channels. Output shape for each: {left_channel_wf.shape}")

        # 3. Package each channel into its own ComfyUI AUDIO dict.
        audio_left = {
            "waveform": left_channel_wf,
            "sample_rate": sample_rate
        }
        audio_right = {
            "waveform": right_channel_wf,
            "sample_rate": sample_rate
        }

        return (audio_left, audio_right)


class AudioConcatenate:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio1": ("AUDIO", {"tooltip": "The first audio clip (or batch)."}),
                "audio2": ("AUDIO", {"tooltip": "The second audio clip (or batch) to append."}),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "concatenate_audio"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Concatenates two audio signals end-to-end in time."
    UNIQUE_NAME = "SET_AudioConcatenate"
    DISPLAY_NAME = "Audio Concatenate"

    def concatenate_audio(self, audio1: dict, audio2: dict):
        # 1. Align the two audio inputs. This will unify their sample rate and channel count.
        # It will also pad their *lengths* to be equal, which is not what we want for concatenation.
        # We will use the aligned waveforms but ignore the padding by using their original lengths
        # after resampling.

        aligner = AudioBatchAligner(audio1, audio2)
        # aligned_wf1 is (B1, C_target, N_target), aligned_wf2 is (B2, C_target, N_target)
        aligned_wf1, aligned_wf2, target_sr = aligner.get_aligned_waveforms()

        # 2. Determine the original lengths *after resampling* to know how much to take from each.
        n1_orig = audio1['waveform'].shape[2]
        n2_orig = audio2['waveform'].shape[2]
        n1_after_resample = (n1_orig if audio1['sample_rate'] == target_sr else
                             int(n1_orig * (target_sr / audio1['sample_rate'])))
        n2_after_resample = (n2_orig if audio2['sample_rate'] == target_sr else
                             int(n2_orig * (target_sr / audio2['sample_rate'])))

        # Take the un-padded, aligned data from each waveform
        wf1_to_concat = aligned_wf1[..., :n1_after_resample]
        wf2_to_concat = aligned_wf2[..., :n2_after_resample]

        # 3. Handle mismatched batch sizes by repeating the last item.
        b1, b2 = wf1_to_concat.shape[0], wf2_to_concat.shape[0]
        if b1 != b2:
            if b1 < b2:
                last_item = wf1_to_concat[-1:, :, :]  # Keep batch dim
                repeats_needed = b2 - b1
                wf1_to_concat = torch.cat([wf1_to_concat, last_item.repeat(repeats_needed, 1, 1)], dim=0)
            else:  # b2 < b1
                last_item = wf2_to_concat[-1:, :, :]
                repeats_needed = b1 - b2
                wf2_to_concat = torch.cat([wf2_to_concat, last_item.repeat(repeats_needed, 1, 1)], dim=0)

        # Now both wf1_to_concat and wf2_to_concat are (max(B1,B2), C_target, N_relevant)

        # 4. Concatenate along the time/samples dimension (dim=2)
        concatenated_waveform = torch.cat((wf1_to_concat, wf2_to_concat), dim=2)

        logger.info(f"Concatenated audio. Final shape: {concatenated_waveform.shape}, SR: {target_sr}")

        output_audio = {
            "waveform": concatenated_waveform,
            "sample_rate": target_sr
        }
        return (output_audio,)


class AudioNormalize:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO", {"tooltip": "The audio to normalize."}),
                "peak_level": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 10.0,  # Allow some headroom if needed
                    "step": 0.01,
                    "tooltip": "The target peak amplitude level. 1.0 is 0 dBFS (maximum)."
                })
            },
        }

    RETURN_TYPES = ("AUDIO", "TORCH_TENSOR")
    RETURN_NAMES = ("normalized_audio", "original_peak_level")
    FUNCTION = "normalize_audio"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Normalizes audio so its loudest peak reaches a target level."
    UNIQUE_NAME = "SET_AudioNormalize"
    DISPLAY_NAME = "Audio Normalize (Peak)"

    def normalize_audio(self, audio: dict, peak_level: float):
        waveform = audio['waveform']
        sample_rate = audio['sample_rate']

        logger.debug(f"Normalizing audio of shape {waveform.shape} to target peak: {peak_level}")

        # Find the maximum absolute value for each batch item.
        # This is the original peak level we need to return.
        original_peak_level, _ = torch.max(torch.abs(waveform), dim=2, keepdim=True)
        original_peak_level, _ = torch.max(original_peak_level, dim=1, keepdim=True)
        # original_peak_level shape is (B, 1, 1)

        # Define a threshold for what we consider silent to avoid division by zero
        silence_threshold = 1e-8

        # Calculate the gain factor needed to reach the target peak_level.
        # Add the threshold to the denominator to prevent division by zero for silent clips.
        gain_factor = peak_level / (original_peak_level + silence_threshold)

        # For silent clips (where original_peak_level is below threshold), the gain
        # will be huge but will be multiplied by ~0, resulting in ~0 (silence).
        # We can explicitly set gain to 0 for silent clips to be safer.
        is_silent = original_peak_level <= silence_threshold
        gain_factor[is_silent] = 0.0

        # Apply the gain
        normalized_waveform = waveform * gain_factor

        # The value needed to revert is the original peak level.
        # Squeeze it to be a more convenient shape for other nodes (e.g., a 1D tensor or a scalar if B=1)
        # Squeeze removes all dims of size 1. (B,1,1) -> (B)
        revert_value = original_peak_level.squeeze()
        # If the original batch size was 1, this will be a 0-dim tensor (scalar).
        # Let's ensure it's at least a 1D tensor for consistency.
        if revert_value.ndim == 0:
            revert_value = revert_value.unsqueeze(0)  # Make it (1,)

        logger.info(f"Peak normalization applied. Original avg peak: {original_peak_level.mean():.4f}, "
                    f"Target peak: {peak_level}")

        output_audio = {"waveform": normalized_waveform, "sample_rate": sample_rate}

        # The revert_value is the original peak level for each item in the batch
        return (output_audio, revert_value)


class AudioApplyBatchedGain:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO", {"tooltip": "The audio batch to apply gain to."}),
                "gain_values": ("TORCH_TENSOR", {
                    "tooltip": "A batch of gain values (e.g., from an Audio Normalize node). Expects a 1D tensor."
                }),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "apply_gain"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Applies a different gain (volume) to each item in an audio batch."
    UNIQUE_NAME = "SET_AudioApplyBatchedGain"
    DISPLAY_NAME = "Audio Apply Batched Gain"

    def apply_gain(self, audio: dict, gain_values: torch.Tensor):
        waveform = audio['waveform']
        sample_rate = audio['sample_rate']

        batch_size = waveform.shape[0]

        # Ensure gain_values is a tensor
        if not isinstance(gain_values, torch.Tensor):
            # If a simple float is passed, convert it to a tensor
            # This makes the node more flexible
            gain_values = torch.tensor([gain_values], dtype=torch.float32)

        # Validate dimensions
        if gain_values.ndim == 0:  # Handle scalar tensor
            gain_values = gain_values.unsqueeze(0)

        if gain_values.shape[0] != batch_size:
            msg = (f"Batch size of audio ({batch_size}) and gain_values ({gain_values.shape[0]}) must match. "
                   "Connect the `original_peak_level` from an Audio Normalize node.")
            logger.error(msg)
            raise ValueError(msg)

        # Reshape gain_values for broadcasting: (B) -> (B, 1, 1)
        # This allows multiplying (B, C, N) with (B, 1, 1)
        gain_tensor = gain_values.to(waveform.device, waveform.dtype).reshape(batch_size, 1, 1)

        # Apply the per-item gain
        adjusted_waveform = waveform * gain_tensor

        logger.info(f"Applied batched gain. Gain range: [{gain_values.min():.4f}, {gain_values.max():.4f}]")

        output_audio = {
            "waveform": adjusted_waveform,
            "sample_rate": sample_rate
        }
        return (output_audio,)


class AudioDownload:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "base_url": ("STRING", {
                    "default": "https://huggingface.co/set-soft/audio_separation/resolve/main/Audio_Examples/",
                    "tooltip": "The base URL or directory path where the audio file is located."
                }),
                "filename": ("STRING", {
                    "default": "01-The_Symphony_of_Automation.mp3",
                    "tooltip": "The name of the audio file to download (e.g., music.mp3, effect.wav)."
                }),
                "target_sample_rate": ("INT", {
                    "default": 0, "min": 0, "max": 192000, "step": 100,
                    "tooltip": "Target sample rate to load the audio at. 0 means use the file's original sample rate."
                }),
                # This is a magic input that will be converted to the audio player by ComfyUI frontend
                "audioUI": ("AUDIO_UI", {}),
            },
            "optional": {
                "audio_bypass": ("AUDIO", {"tooltip": "If this audio is present will be used instead of the downloaded one"}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio",)
    FUNCTION = "load_or_download_audio"
    CATEGORY = BASE_CATEGORY + "/" + IO_CATEGORY
    DESCRIPTION = "Downloads an audio file to the ComfyUI 'input' directory if it doesn't exist, then loads it."
    UNIQUE_NAME = "SET_AudioDownload"
    DISPLAY_NAME = "Audio Download and Load"
    OUTPUT_NODE = True

    def load_or_download_audio(self, base_url: str, filename: str, target_sample_rate: int, audioUI: str,
                               audio_bypass: Optional[Dict[str, Any]] = None):
        if audio_bypass:
            return (audio_bypass,)
        # Determine the save directory: ComfyUI's input directory
        save_dir = get_input_directory()
        local_filepath = os.path.join(save_dir, filename)

        # 1. Check if the file already exists locally
        if not os.path.exists(local_filepath):
            logger.info(f"File '{filename}' not found locally. Attempting to download.")

            if not base_url.endswith('/'):
                base_url += '/'
            download_url = base_url + filename
            send_toast_notification(f"Downloading `{filename}`", "Download")

            try:
                download_model(url=download_url, save_dir=save_dir, file_name=filename, kind="audio")
            except Exception as e:
                logger.error(f"Download failed for {download_url}: {e}", exc_info=True)
                raise  # Re-raise to stop the workflow and show the error
            send_toast_notification("Finished downloading", "Download", 'success')
        else:
            logger.info(f"Found existing file, skipping download: '{local_filepath}'")

        # 2. Load the audio file using torchaudio
        try:
            # `torchaudio.load` returns (waveform, original_sample_rate)
            waveform, original_sr = torchaudio.load(local_filepath)

            # Waveform from torchaudio.load is (channels, samples)
            logger.info(f"Loaded '{filename}' with original SR: {original_sr} Hz, shape: {waveform.shape}")

            # 3. Resample if necessary
            if target_sample_rate > 0 and original_sr != target_sample_rate:
                logger.info(f"Resampling from {original_sr} Hz to {target_sample_rate} Hz.")
                resampler = T.Resample(orig_freq=original_sr, new_freq=target_sample_rate, dtype=waveform.dtype,
                                       lowpass_filter_width=24)
                waveform = resampler(waveform)
                loaded_sr = target_sample_rate
            else:
                loaded_sr = original_sr

            # 4. Convert to ComfyUI's standard format (B, C, N)
            # Add a batch dimension to the (C, N) waveform.
            waveform_batched = waveform.unsqueeze(0).float()

            output_audio = {
                "waveform": waveform_batched,
                "sample_rate": loaded_sr
            }
            # This is the information needed to update the audio widget (player)
            downloaded_file = {
                 "audio": [{
                     "filename": filename,
                     "subfolder": "",
                     "type": "input"  # We stored the file in the "input" folder
                 }]
            }
            return {"ui":  downloaded_file, "result": [output_audio]}

        except Exception as e:
            logger.error(f"Failed to load or process audio file '{local_filepath}': {e}", exc_info=True)
            raise IOError(f"Could not load or process the audio file: {filename}. It may be missing or corrupt.") from e
