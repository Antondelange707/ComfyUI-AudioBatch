# -*- coding: utf-8 -*-
# Copyright (c) 2025 Salvador E. Tropea
# Copyright (c) 2025 Instituto Nacional de Tecnolog√Øa Industrial
# License: GPL-3.0
# Project: ComfyUI-AudioBatch
# From code generated by Gemini 2.5 Pro
import torch
import torchaudio.transforms as T
from .utils.aligner import AudioBatchAligner
from .utils.logger import main_logger
from .utils.misc import parse_time_to_seconds

logger = main_logger
BASE_CATEGORY = "audio"
BATCH_CATEGORY = "batch"
CONV_CATEGORY = "conversion"
MANIPULATION_CATEGORY = "manipulation"


class AudioBatch:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio1": ("AUDIO", {"tooltip": "The first audio input. Can be a single audio item or a batch"}),
                "audio2": ("AUDIO", {"tooltip": "The second audio input. Can be a single audio item or a batch"}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_batch",)
    FUNCTION = "batch_audio"
    CATEGORY = BASE_CATEGORY + "/" + BATCH_CATEGORY
    DESCRIPTION = "Audio batch creator"
    UNIQUE_NAME = "SET_AudioBatch"
    DISPLAY_NAME = "Batch Audios"

    def batch_audio(self, audio1: dict, audio2: dict):
        # 1. Instantiate the aligner
        aligner = AudioBatchAligner(audio1, audio2)
        # 2. Get the aligned waveforms
        aligned_wf1, aligned_wf2, target_sr = aligner.get_aligned_waveforms()
        # 3. Concatenate
        batched_waveform_final = torch.cat((aligned_wf1, aligned_wf2), dim=0)

        logger.info(f"Final batched audio: shape={batched_waveform_final.shape}, sr={target_sr}")

        output_audio = {"waveform": batched_waveform_final, "sample_rate": target_sr}
        return (output_audio,)


class SelectAudioFromBatch:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio_batch": ("AUDIO",),  # Expects {'waveform': (B, C, N), 'sample_rate': int}
                "index": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 0xffffffffffffffff,  # Effectively unbounded, but UI might cap
                    "step": 1,
                    "display": "number",
                    "tootip": "The 0-based index of the audio stream to select from the batch"
                }),
                "behavior_out_of_range": (["silence_original_length", "silence_fixed_length", "error"], {
                    "default": "silence_original_length",
                    "tootip": ("silence_original_length: Output silent audio with the same channel count and duration as "
                               "items in the original batch.\n"
                               "silence_fixed_length: Output silent audio with a duration specified by "
                               " silence_duration_seconds.\n"
                               "error: Raise an error (which will halt the workflow and display an error in ComfyUI)")
                }),
                "silence_duration_seconds": ("FLOAT", {  # Only used if behavior is "silence_fixed_length"
                    "default": 1.0,
                    "min": 0.01,
                    "max": 3600.0,  # 1 hour
                    "step": 0.1,
                    "display": "number",
                    "tootip": "The duration of the silent audio if `behavior_out_of_range` is set to `silence_fixed_length`"
                }),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("selected_audio",)
    FUNCTION = "select_audio"
    CATEGORY = BASE_CATEGORY + "/" + BATCH_CATEGORY
    DESCRIPTION = "Selects an audio from a batch"
    UNIQUE_NAME = "SET_SelectAudioFromBatch"
    DISPLAY_NAME = "Select Audio from Batch"

    def select_audio(self, audio_batch: dict, index: int, behavior_out_of_range: str, silence_duration_seconds: float):

        waveform_batch = audio_batch['waveform']  # (B, C, N)
        sample_rate = audio_batch['sample_rate']

        batch_size, num_channels, num_samples_per_item = waveform_batch.shape

        logger.debug(f"Input audio batch: shape={waveform_batch.shape}, sr={sample_rate}, index={index}")
        logger.debug(f"Out of range behavior: {behavior_out_of_range}, silence duration: {silence_duration_seconds}s")

        selected_waveform = None

        if 0 <= index < batch_size:
            # Valid index, select the audio
            # Slicing with index:index+1 keeps the batch dimension, resulting in (1, C, N)
            selected_waveform = waveform_batch[index:index+1, :, :]
            logger.info(f"Selected audio at index {index}. Shape: {selected_waveform.shape}")
        else:
            # Index is out of range
            msg = f"Index {index} is out of range for batch of size {batch_size}."
            logger.warning(msg)

            if behavior_out_of_range == "error":
                raise ValueError(msg)
            elif behavior_out_of_range == "silence_original_length":
                logger.info(f"Outputting silence with original length: {num_samples_per_item} samples, "
                            f"{num_channels} channels.")
                selected_waveform = torch.zeros((1, num_channels if num_channels > 0 else 1, num_samples_per_item),
                                                dtype=waveform_batch.dtype, device=waveform_batch.device)
            elif behavior_out_of_range == "silence_fixed_length":
                silence_samples = int(silence_duration_seconds * sample_rate)
                if silence_samples <= 0:  # Ensure positive sample count
                    silence_samples = 1
                    logger.warning(f"Calculated silence samples is <=0 ({silence_samples} from "
                                   f"{silence_duration_seconds}s). Using 1 sample.")
                logger.info(f"Outputting silence with fixed length: {silence_samples} samples, {num_channels} channels.")
                selected_waveform = torch.zeros((1, num_channels if num_channels > 0 else 1, silence_samples),
                                                dtype=waveform_batch.dtype, device=waveform_batch.device)

        if selected_waveform is None:  # Should not happen if logic above is complete
            logger.error("Selected waveform is None unexpectedly. Defaulting to silence.")
            selected_waveform = torch.zeros((1, num_channels if num_channels > 0 else 1, 1),
                                            dtype=waveform_batch.dtype, device=waveform_batch.device)

        output_audio = {
            "waveform": selected_waveform,  # Shape (1, C, N_selected)
            "sample_rate": sample_rate
        }

        return (output_audio,)


class AudioChannelConverter:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "channel_conversion": (["keep", "stereo_to_mono", "mono_to_stereo", "force_mono", "force_stereo"],
                                       {"default": "keep",
                                        "tooltip": "keep: maintain same channels,\n"
                                                   "stereo_to_mono/force_mono: 1 channel,\n"
                                                   "mono_to_stereo/force_stereo: 2 channels"}),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "convert_channels"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Converts audio channels (mono/stereo/multi-channel handling)."
    UNIQUE_NAME = "SET_AudioChannelConverter"
    DISPLAY_NAME = "Audio Channel Converter"

    def convert_channels(self, audio: dict, channel_conversion: str):
        waveform = audio['waveform']  # (B, C, T)
        sample_rate = audio['sample_rate']

        original_batch_size, original_channels, original_samples = waveform.shape
        logger.debug(f"Input audio: {original_batch_size}B, {original_channels}C, {original_samples}T @ {sample_rate}Hz")
        logger.debug(f"Channel conversion mode: {channel_conversion}")

        output_waveform = waveform.clone()  # Start with a copy

        if channel_conversion == "keep":
            # No change needed, but log if > 2 channels
            if original_channels > 2:
                logger.warning(f"Channel mode 'keep': Input has {original_channels} channels. Outputting all "
                               f"{original_channels} channels.")
            # output_waveform remains as is

        elif channel_conversion == "stereo_to_mono" or channel_conversion == "force_mono":
            if original_channels == 1:
                logger.debug("Input is already mono. No change for stereo_to_mono/force_mono.")
                # output_waveform remains as is
            else:  # Stereo or Multi-channel to Mono
                if original_channels > 2 and channel_conversion == "stereo_to_mono":
                    logger.warning(f"Channel mode 'stereo_to_mono': Input has {original_channels} channels. "
                                   "Averaging all to mono.")
                elif channel_conversion == "force_mono":
                    logger.info(f"Channel mode 'force_mono': Input has {original_channels} channels. Averaging all to mono.")
                # Average across the channel dimension (dim=1)
                output_waveform = torch.mean(waveform, dim=1, keepdim=True)
                logger.debug(f"Converted to mono. New shape: {output_waveform.shape}")

        elif channel_conversion == "mono_to_stereo" or channel_conversion == "force_stereo":
            if original_channels == 2:
                logger.debug("Input is already stereo. No change for mono_to_stereo/force_stereo.")
                # output_waveform remains as is
            elif original_channels == 1:  # Mono to Stereo
                # Duplicate the mono channel
                output_waveform = waveform.repeat(1, 2, 1)
                logger.debug(f"Converted mono to stereo by duplication. New shape: {output_waveform.shape}")
            else:  # Multi-channel (>2) to Stereo
                if channel_conversion == "mono_to_stereo":
                    logger.warning(f"Channel mode 'mono_to_stereo': Input has {original_channels} channels. "
                                   "Taking the first channel and duplicating it to create stereo.")
                elif channel_conversion == "force_stereo":
                    logger.info(f"Channel mode 'force_stereo': Input has {original_channels} channels. "
                                "Taking the first channel and duplicating it to create stereo.")
                output_waveform = waveform[:, 0:1, :].repeat(1, 2, 1)  # Take first channel, make it (B,1,T), then repeat
                logger.debug(f"Converted {original_channels}ch to stereo. New shape: {output_waveform.shape}")
        else:
            logger.error(f"Unknown channel_conversion mode: {channel_conversion}. Returning original.")
            # output_waveform remains as is (original waveform)

        processed_audio_dict = {"waveform": output_waveform, "sample_rate": sample_rate}
        return (processed_audio_dict,)


class AudioResampler:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "target_sample_rate": ("INT", {
                    "default": 0, "min": 0, "max": 192000, "step": 100, "tooltip": "Output sample rate, 0 is same as input"}),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "resample_audio"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Resamples audio to a target sample rate using torchaudio."
    UNIQUE_NAME = "SET_AudioResampler"
    DISPLAY_NAME = "Audio Resampler"

    def resample_audio(self, audio: dict, target_sample_rate: int):
        waveform = audio['waveform']  # (B, C, T)
        original_sample_rate = audio['sample_rate']

        logger.debug(f"Input audio SR: {original_sample_rate}Hz, Target SR: {target_sample_rate}Hz")

        if target_sample_rate == 0 or target_sample_rate == original_sample_rate:
            logger.info(f"Target sample rate ({target_sample_rate}Hz) is 0 or matches original ({original_sample_rate}Hz). "
                        "Skipping resampling.")
            return (audio,)  # Return original audio dict

        try:
            # Ensure waveform is on the CPU for torchaudio transforms if it might be on GPU
            # Though many torchaudio ops support GPU tensors. Resample does.
            # For consistency or if issues arise:
            # device = waveform.device
            # waveform_cpu = waveform.cpu()
            # resampler = T.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate).to(device)
            # resampled_waveform = resampler(waveform)
            resampler = T.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate,
                                   dtype=waveform.dtype,  # Preserve dtype
                                   lowpass_filter_width=24
                                   ).to(waveform.device)  # Perform resampling on the tensor's current device

            resampled_waveform = resampler(waveform)
            logger.info(f"Resampling audio from {original_sample_rate}Hz to {target_sample_rate}Hz. "
                        f"Original shape: {waveform.shape}, New shape: {resampled_waveform.shape}")
            processed_audio_dict = {"waveform": resampled_waveform, "sample_rate": target_sample_rate}
            return (processed_audio_dict,)

        except Exception as e:
            logger.error(f"Error during resampling: {e}", exc_info=True)
            # Fallback: return original audio if resampling fails
            return (audio,)


class AudioProcessAdvanced:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "channel_conversion": (["keep", "stereo_to_mono", "mono_to_stereo", "force_mono", "force_stereo"],
                                       {"default": "keep",
                                        "tooltip": "keep: maintain same channels,\n"
                                                   "stereo_to_mono/force_mono: 1 channel,\n"
                                                   "mono_to_stereo/force_stereo: 2 channels"}),
                "target_sample_rate": ("INT", {"default": 0, "min": 0, "max": 192000, "step": 100.,
                                               "tooltip": "Output sample rate, 0 is same as input"}),
            },
        }
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "process_audio"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Applies channel conversion and resampling to audio."
    UNIQUE_NAME = "SET_AudioChannelConvResampler"
    DISPLAY_NAME = "Audio Channel Conv and Resampler"

    def process_audio(self, audio: dict, channel_conversion: str, target_sample_rate: int):
        # Instantiate helper classes (or move their logic directly here)
        channel_converter_node = AudioChannelConverter()
        resampler_node = AudioResampler()

        # 1. Channel Conversion
        (audio_after_channels,) = channel_converter_node.convert_channels(audio, channel_conversion)

        # 2. Resampling
        (audio_after_resample,) = resampler_node.resample_audio(audio_after_channels, target_sample_rate)

        return (audio_after_resample,)


class AudioInfo:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
            },
        }

    RETURN_TYPES = ("AUDIO", "INT", "INT", "INT", "INT")
    RETURN_NAMES = ("audio_bypass", "batch_size", "channels", "num_samples", "sample_rate")
    FUNCTION = "show_info"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Shows information about the audio."
    UNIQUE_NAME = "SET_AudioInfo"
    DISPLAY_NAME = "Audio Information"

    def show_info(self, audio: dict):
        wav = audio['waveform']  # (B, C, T)
        sample_rate = audio['sample_rate']
        return audio, wav.shape[0], wav.shape[1], wav.shape[2], sample_rate


class AudioForceChannels:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "channels": ("INT", {"default": 0, "min": 0, "max": 2}),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio",)
    FUNCTION = "force_channels"
    CATEGORY = BASE_CATEGORY + "/" + CONV_CATEGORY
    DESCRIPTION = "Forces the number of channels. 0 means keep same."
    UNIQUE_NAME = "SET_AudioForceChannels"
    DISPLAY_NAME = "Audio Force Channels"
    CHANNELS_TO_MODE = ["keep", "force_mono", "force_stereo"]

    def force_channels(self, audio: dict, channels: int):
        return AudioChannelConverter().convert_channels(audio, self.CHANNELS_TO_MODE[channels])


class AudioCut:
    """ From https://github.com/christian-byrne/audio-separation-nodes-comfyui/ """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio": ("AUDIO",),
                "start_time": ("STRING", {
                    "default": "0:00",
                    "tooltip": "Start time in HH:MM:SS.ss format, or just a float."
                }),
                "end_time": ("STRING", {
                    "default": "1:00",
                    "tooltip": "End time in HH:MM:SS.ss format, or just a float."
                }),
            },
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "cut"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Cuts a portion of the input audio. Can use seconds or HH:MM:SS.ss."
    UNIQUE_NAME = "SET_AudioCut"
    DISPLAY_NAME = "Audio Cut"

    def cut(self, audio: dict, start_time: str, end_time: str):
        waveform = audio["waveform"]
        sample_rate = audio["sample_rate"]
        length = waveform.shape[-1] - 1

        start_frame = int(parse_time_to_seconds(start_time) * sample_rate)
        start_frame = max(0, min(start_frame, length))

        end_frame = int(parse_time_to_seconds(end_time) * sample_rate)
        end_frame = max(0, min(end_frame, length))

        logger.debug(f"Cutting audio from {start_frame} to {end_frame} (SR: {sample_rate})")
        if start_frame > end_frame:
            raise ValueError("Audio Cut: Start time must be smaller than end time and both be within the audio length.")

        return ({"waveform": waveform[..., start_frame:end_frame],
                 "sample_rate": sample_rate},)


class AudioBlend:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio1": ("AUDIO", {"tooltip": "The first audio input (batch supported)."}),
                "gain1": ("FLOAT", {"default": 1.0, "min": -10.0, "max": 10.0, "step": 0.01,
                                    "tooltip": "Volume gain for the first audio. Can be negative to subtract."}),
                "gain2": ("FLOAT", {"default": 1.0, "min": -10.0, "max": 10.0, "step": 0.01,
                                    "tooltip": "Volume gain for the second audio. Can be negative to subtract."}),
            },
            "optional": {
                "audio2": ("AUDIO", {"tooltip": "The second audio input (optional, batch supported)."}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio_out",)
    FUNCTION = "blend_audio"
    CATEGORY = BASE_CATEGORY + "/" + MANIPULATION_CATEGORY
    DESCRIPTION = "Blends two audio inputs by applying gain and adding them. Supports batches."
    UNIQUE_NAME = "SET_AudioBlend"
    DISPLAY_NAME = "Audio Blend"

    def blend_audio(self, audio1: dict, gain1: float, gain2: float, audio2: dict = None):
        if audio2 is None:
            # Handle the simple case where audio2 is not provided
            logger.info(f"Blending audio1 only with gain {gain1}.")
            blended_waveform = audio1['waveform'] * gain1
            return ({"waveform": blended_waveform, "sample_rate": audio1['sample_rate']},)

        # If audio2 is provided, align both audio inputs
        # 1. Instantiate the aligner with the logger
        aligner = AudioBatchAligner(audio1, audio2)

        # 2. Get the aligned waveforms
        aligned_wf1, aligned_wf2, target_sr = aligner.get_aligned_waveforms()

        b1, c, n = aligned_wf1.shape
        b2 = aligned_wf2.shape[0]

        # 3. Perform the blend operation on aligned tensors
        target_batch_size = max(b1, b2)
        blended_waveform = torch.zeros(target_batch_size, c, n, dtype=aligned_wf1.dtype, device=aligned_wf1.device)

        logger.info(f"Blending {b1} items from audio1 with {b2} items from audio2 into a batch of {target_batch_size}.")

        # Add the scaled audio streams to the output tensor
        # This correctly handles mismatched batch sizes by only adding where data exists.
        blended_waveform[:b1] += aligned_wf1 * gain1
        blended_waveform[:b2] += aligned_wf2 * gain2

        output_audio = {"waveform": blended_waveform, "sample_rate": target_sr}
        return (output_audio,)
